â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                   â•‘
â•‘           WHAT THE USER MUST DO MANUALLY - CHECKLIST              â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

This document outlines all manual steps required to get FinBot running in your environment.


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. ENVIRONMENT SETUP (Required)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¡ Install Python 3.10 or higher
  Download from: https://www.python.org/downloads/
  Verify: python --version

â–¡ Install pip (Python package manager)
  Usually included with Python
  Verify: pip --version

â–¡ Create virtual environment
  
  Linux/macOS:
  ```
  python3 -m venv venv
  source venv/bin/activate
  ```
  
  Windows:
  ```
  python -m venv venv
  venv\Scripts\activate
  ```

â–¡ Install project dependencies
  
  ```
  pip install -r requirements.txt
  ```
  
  This installs:
  - fastapi, uvicorn (backend)
  - streamlit (frontend)
  - torch, transformers (LLMs)
  - sentence-transformers, faiss-cpu (embeddings)
  - python-dotenv, pydantic (configuration)
  - PyPDF2 (document processing)
  - Additional supporting libraries

â–¡ Verify installations
  
  ```
  python -c "import fastapi, streamlit, torch, transformers, faiss; print('âœ“ All imports successful')"
  ```


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2. CONFIGURATION SETUP (Important)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¡ Create .env file from template
  
  ```
  cp .env.example .env
  ```
  
  On Windows:
  ```
  copy .env.example .env
  ```

â–¡ Edit .env file with your settings
  
  Open .env in text editor and configure:
  
  BASIC SETTINGS (can keep defaults):
  â”œâ”€ API_HOST=0.0.0.0              (server address)
  â”œâ”€ API_PORT=8000                 (backend port)
  â”œâ”€ EMBEDDING_MODEL=all-MiniLM-L6-v2 (fast embeddings)
  â”œâ”€ CHUNK_SIZE=500                (document chunk size)
  â”œâ”€ TOP_K_DOCUMENTS=5             (retrieval count)
  â””â”€ DEVICE=cpu                    (or cuda for GPU)
  
  CHOOSE LLM (one of the following):
  
  Option A - HuggingFace (Default, Recommended)
  â”œâ”€ USE_OPENAI=false
  â”œâ”€ HF_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.1
  â””â”€ HF_API_TOKEN=                 (leave empty or get from huggingface.co)
  
  Option B - OpenAI (Requires API key)
  â”œâ”€ USE_OPENAI=true
  â”œâ”€ OPENAI_API_KEY=sk_XXXXXXXXX   (get from openai.com)
  â””â”€ OPENAI_MODEL=gpt-3.5-turbo

â–¡ If using OpenAI:
  
  1. Visit: https://platform.openai.com/account/api-keys
  2. Create new API key
  3. Copy and paste into OPENAI_API_KEY in .env
  4. Keep this key SECRET and never commit to git

â–¡ If using HuggingFace:
  
  1. (Optional) Visit: https://huggingface.co/settings/tokens
  2. Create access token
  3. Paste into HF_API_TOKEN if using gated models
  4. Large models will auto-download on first run (~7GB for Mistral)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
3. PROJECT STRUCTURE INITIALIZATION (Automatic, Verify)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The following directories are created automatically, but verify they exist:

â–¡ data/documents/
  Purpose: Place your PDF/TXT files here
  Example files to add:
  â”œâ”€ finance_guide.pdf
  â”œâ”€ investment_basics.txt
  â””â”€ market_overview.pdf

â–¡ embeddings/
  Purpose: Stores FAISS vector index (auto-created)
  Files generated:
  â”œâ”€ faiss_index.index (vector database)
  â”œâ”€ faiss_index_metadata.pkl (metadata)
  â””â”€ faiss_index_documents.pkl (document chunks)

â–¡ logs/
  Purpose: Application logs
  Files generated:
  â””â”€ finbot.log (when app runs)

â–¡ config/
  Purpose: Already populated with settings.py

Create if missing:
```
mkdir -p data/documents embeddings logs
```


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
4. ADD FINANCIAL DOCUMENTS (Important)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The chatbot works best with documents. To add them:

â–¡ Prepare documents
  
  Supported formats:
  â”œâ”€ PDF files (.pdf)
  â””â”€ Text files (.txt)
  
  Example topics:
  â”œâ”€ Financial concepts (compound interest, diversification)
  â”œâ”€ Investment guides (stocks, bonds, ETFs)
  â”œâ”€ Risk management
  â”œâ”€ Retirement planning
  â”œâ”€ Market analysis
  â””â”€ Personal finance tips

â–¡ Add documents to data/documents/
  
  Copy your files:
  ```
  cp my_finance_guide.pdf data/documents/
  cp investment_basics.txt data/documents/
  ```

â–¡ (Optional) Use provided sample
  
  The project includes:
  â””â”€ data/financial_qa.json (10 Q&A pairs for fine-tuning)

â–¡ (Optional) Create sample TXT file for testing
  
  Create data/documents/sample.txt with content like:
  ```
  Compound Interest
  ================
  Compound interest is the eighth wonder of the world.
  Those who understand it earn it; those who don't pay it.
  It's calculated as A = P(1 + r/n)^(nt) where...
  ```

Note: Without documents, the chatbot will still work but needs at least 
one document to provide meaningful context. The backend will load 
documents on startup from data/documents/.


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
5. RUNNING THE APPLICATION (Three Options)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

IMPORTANT: Ensure virtual environment is ACTIVATED before running!

Activate venv:
  Linux/macOS: source venv/bin/activate
  Windows: venv\Scripts\activate

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

OPTION 1: FULL STACK (Backend + Frontend, Recommended for Development)

Step 1 - Start Backend API (Terminal 1)
  ```
  python -m backend.api
  ```
  
  Expected output:
  â”œâ”€ Loading embedding model...
  â”œâ”€ Initializing RAG pipeline...
  â”œâ”€ Loading documents from data/documents
  â”œâ”€ INFO: Uvicorn running on http://0.0.0.0:8000
  â””â”€ INFO: Application startup complete
  
  Verify: Open browser to http://localhost:8000/health
  Should show: {"status": "healthy", ...}

Step 2 - Start Frontend (Terminal 2)
  ```
  streamlit run frontend/app.py
  ```
  
  Expected output:
  â”œâ”€ Collecting usage statistics
  â”œâ”€ You can now view your Streamlit app in your browser
  â””â”€ URL: http://localhost:8501
  
  Browser auto-opens to http://localhost:8501
  If not, visit: http://localhost:8501

Step 3 - Use the application
  â”œâ”€ Type question in chat input
  â”œâ”€ Click upload to add documents
  â”œâ”€ View sources with relevance scores
  â””â”€ Check status in sidebar

Stop: Press Ctrl+C in both terminals

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

OPTION 2: API ONLY (Backend Only, for Integration)

Run backend:
  ```
  python -m backend.api
  ```
  
  API endpoints available at:
  â”œâ”€ http://localhost:8000/health
  â”œâ”€ http://localhost:8000/docs (Swagger UI)
  â”œâ”€ http://localhost:8000/chat (POST)
  â”œâ”€ http://localhost:8000/status
  â””â”€ http://localhost:8000/upload

Test with curl:
  ```
  curl http://localhost:8000/health
  curl -X POST http://localhost:8000/chat \
    -H "Content-Type: application/json" \
    -d '{"query": "What is diversification?"}'
  ```

Or use Python:
  ```python
  import requests
  response = requests.post(
    "http://localhost:8000/chat",
    json={"query": "What is compound interest?"}
  )
  print(response.json())
  ```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

OPTION 3: DOCKER (Containerized, for Production/Deployment)

Requirements:
  â”œâ”€ Docker installed (https://www.docker.com/products/docker-desktop)
  â””â”€ Docker daemon running

Run with docker-compose:
  ```
  docker-compose up -d
  ```
  
  This starts:
  â”œâ”€ API on http://localhost:8000
  â””â”€ Frontend on http://localhost:8501
  
  View logs:
  ```
  docker-compose logs -f api
  docker-compose logs -f frontend
  ```
  
  Stop:
  ```
  docker-compose down
  ```

Build custom image:
  ```
  docker build -t finbot:latest .
  docker run -p 8000:8000 -p 8501:8501 \
    -e USE_OPENAI=false \
    -v $(pwd)/data:/app/data \
    finbot:latest
  ```


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
6. DEPLOYMENT (For Production)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Choose one platform:

â–¡ Streamlit Cloud (Recommended for Frontend)
  
  1. Push code to GitHub
  2. Go to share.streamlit.io
  3. Create new app from your GitHub repo
  4. Select frontend/app.py as main file
  5. Add secrets (API keys) in app settings
  6. Share your app URL

â–¡ HuggingFace Spaces (Free Backend)
  
  1. Create Space on huggingface.co
  2. Push code to Space
  3. Configure environment variables
  4. App auto-deploys

â–¡ AWS (Scalable)
  
  Lambda + API Gateway: serverless backend
  Elastic Beanstalk: managed backend
  S3 + CloudFront: static frontend
  See deploy.md for details

â–¡ Google Cloud Platform
  
  Cloud Run: serverless backend
  Firebase Hosting: frontend
  Cloud Storage: documents
  See deploy.md for details

â–¡ Docker Hub / Any Container Registry
  
  1. Build image: docker build -t finbot:latest .
  2. Tag: docker tag finbot:latest username/finbot:latest
  3. Push: docker push username/finbot:latest
  4. Deploy: Pull and run on any platform

See deploy.md for detailed instructions for each platform.


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
7. OPTIONAL: FINE-TUNING (Advanced)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

To fine-tune the model on financial Q&A:

â–¡ Prepare Q&A dataset
  
  File format: data/financial_qa.json
  
  Structure:
  [
    {
      "question": "What is compound interest?",
      "answer": "Compound interest is...",
      "context": "Financial concepts"
    }
  ]
  
  Note: Sample file is provided in data/financial_qa.json

â–¡ Install fine-tuning dependencies
  
  ```
  pip install peft torch
  ```

â–¡ Run fine-tuning script
  
  ```
  python -c "
  from training.finetune import FineTuner
  fine_tuner = FineTuner(device='cuda')  # or 'cpu'
  fine_tuner.load_model()
  train_loader, val_loader = fine_tuner.prepare_data('data/financial_qa.json')
  history = fine_tuner.fine_tune(train_loader, val_loader, epochs=3)
  fine_tuner.save_model('models/finbot-finetuned')
  "
  ```

â–¡ Use fine-tuned model
  
  Update .env:
  HF_MODEL_NAME=models/finbot-finetuned

Note: Fine-tuning requires GPU and takes 1-2 hours. 
Recommended only for advanced users.


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
8. TESTING CHECKLIST (Verification)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Before considering setup complete, test:

â–¡ Backend Health Check
  
  ```
  curl http://localhost:8000/health
  ```
  Should return: status: "healthy"

â–¡ API Documentation
  
  Visit: http://localhost:8000/docs
  You should see interactive Swagger UI

â–¡ Status Endpoint
  
  ```
  curl http://localhost:8000/status
  ```
  Should show documents_count, device, etc.

â–¡ Frontend Loading
  
  Visit: http://localhost:8501
  Should see chat interface with sidebar

â–¡ Test Chat (With Documents)
  
  1. Upload document (frontend or via API)
  2. Type question: "What is your document about?"
  3. Get response with sources
  
  If no documents:
  Ask: "What is diversification?"
  Should still get generic response

â–¡ Document Upload
  
  Frontend: Use "Upload Documents" in sidebar
  API: curl -X POST http://localhost:8000/upload -F "files=@document.pdf"

â–¡ Source References
  
  After getting response, click "View Sources"
  Should show document names and relevance scores

â–¡ Settings Configuration
  
  Sidebar: Change "Number of Documents" slider
  Submit new question - should retrieve different number of docs

Test Completed âœ“


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
9. TROUBLESHOOTING GUIDE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Issue: "ModuleNotFoundError: No module named 'fastapi'"
Solution:
  â”œâ”€ Check venv is activated
  â”œâ”€ Run: pip install -r requirements.txt
  â””â”€ Verify: pip list | grep fastapi

Issue: "Cannot connect to API at http://localhost:8000"
Solution:
  â”œâ”€ Ensure backend is running (python -m backend.api)
  â”œâ”€ Check port 8000 is not in use
  â”œâ”€ Verify firewall isn't blocking
  â””â”€ Check .env API_PORT setting

Issue: "CUDA out of memory"
Solution:
  â”œâ”€ Use CPU instead: DEVICE=cpu in .env
  â”œâ”€ Reduce CHUNK_SIZE
  â”œâ”€ Reduce FINETUNE_BATCH_SIZE
  â””â”€ Use smaller model: EMBEDDING_MODEL=all-MiniLM-L6-v2

Issue: "Model not found on HuggingFace"
Solution:
  â”œâ”€ Check model name: HF_MODEL_NAME
  â”œâ”€ Login: huggingface-cli login
  â”œâ”€ For gated models, accept terms on HF website

Issue: "No documents in index"
Solution:
  â”œâ”€ Add files to data/documents/
  â”œâ”€ Check file extensions (.pdf or .txt)
  â”œâ”€ Restart backend
  â”œâ”€ Check logs: tail logs/finbot.log

Issue: "Slow inference time"
Solution:
  â”œâ”€ Use GPU: DEVICE=cuda
  â”œâ”€ Reduce TOP_K_DOCUMENTS
  â”œâ”€ Use faster model: HF_MODEL_NAME=google/flan-t5-base
  â”œâ”€ Reduce response length: MAX_TOKENS=256

Issue: ".env file not found"
Solution:
  â”œâ”€ Create from template: cp .env.example .env
  â”œâ”€ Check working directory
  â”œâ”€ Verify file exists: ls .env or dir .env

For more help, see:
  â”œâ”€ README.md - General documentation
  â”œâ”€ deploy.md - Deployment issues
  â”œâ”€ logs/finbot.log - Application logs
  â””â”€ GitHub issues (if using GitHub)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
10. SECURITY CHECKLIST (Important!)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¡ NEVER commit .env to git
  
  Verify .gitignore contains: .env
  Check: git status (should not show .env)

â–¡ NEVER share API keys
  
  â”œâ”€ Keep OPENAI_API_KEY secret
  â”œâ”€ Keep HF_API_TOKEN private
  â”œâ”€ Use environment variables
  â””â”€ Rotate keys if exposed

â–¡ Use HTTPS in production
  
  â”œâ”€ Cloudflare: Free SSL/TLS
  â”œâ”€ Let's Encrypt: Free certificates
  â””â”€ AWS ACM: AWS certificate manager

â–¡ Implement authentication
  
  For production, add:
  â”œâ”€ API key validation
  â”œâ”€ User login (JWT tokens)
  â”œâ”€ Rate limiting
  â””â”€ CORS restrictions

â–¡ Keep dependencies updated
  
  ```
  pip install --upgrade -r requirements.txt
  pip audit  # Check for vulnerabilities
  ```

â–¡ Monitor logs regularly
  
  Check logs/finbot.log for:
  â”œâ”€ Error patterns
  â”œâ”€ Suspicious requests
  â”œâ”€ Performance issues
  â””â”€ Failed authentications

â–¡ Backup your data
  
  â”œâ”€ embeddings/ (FAISS index)
  â”œâ”€ data/documents/ (source files)
  â”œâ”€ .env (keep safe)
  â””â”€ Any custom models in models/


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
11. PERFORMANCE OPTIMIZATION TIPS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

For Better Quality (Slower):
  â”œâ”€ USE_OPENAI=true (or larger HF model)
  â”œâ”€ EMBEDDING_MODEL=all-mpnet-base-v2
  â”œâ”€ CHUNK_SIZE=1000
  â”œâ”€ TOP_K_DOCUMENTS=10
  â””â”€ TEMPERATURE=0.3

For Faster Performance (CPU):
  â”œâ”€ USE_OPENAI=false
  â”œâ”€ HF_MODEL_NAME=google/flan-t5-base
  â”œâ”€ EMBEDDING_MODEL=all-MiniLM-L6-v2
  â”œâ”€ CHUNK_SIZE=250
  â”œâ”€ TOP_K_DOCUMENTS=3
  â””â”€ TEMPERATURE=0.7

For GPU Acceleration:
  â”œâ”€ DEVICE=cuda (requires NVIDIA GPU)
  â”œâ”€ Larger models work well
  â”œâ”€ Batch processing faster
  â””â”€ Install: pip install torch-cuda (based on CUDA version)

For Low Memory:
  â”œâ”€ DEVICE=cpu
  â”œâ”€ EMBEDDING_MODEL=all-MiniLM-L6-v2
  â”œâ”€ CHUNK_SIZE=250
  â”œâ”€ Smaller documents
  â””â”€ BATCH_SIZE=4


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
12. MONITORING & LOGGING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â–¡ Check application logs
  
  ```
  tail -f logs/finbot.log
  ```
  
  Or in Windows:
  ```
  type logs\finbot.log
  ```

â–¡ Monitor API performance
  
  Check /status endpoint:
  ```
  curl http://localhost:8000/status
  ```
  
  Shows:
  â”œâ”€ documents_count: Number of indexed documents
  â”œâ”€ embedding_dimension: Vector size
  â”œâ”€ device: CPU or GPU
  â””â”€ embedding_model: Model being used

â–¡ Check Docker logs
  
  ```
  docker-compose logs -f api
  docker-compose logs -f frontend
  ```

â–¡ Set up monitoring alerts (Production)
  
  â”œâ”€ Datadog: https://www.datadoghq.com/
  â”œâ”€ New Relic: https://newrelic.com/
  â”œâ”€ AWS CloudWatch: https://aws.amazon.com/cloudwatch/
  â””â”€ Google Cloud Monitoring


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SUMMARY - QUICK START COMMANDS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Setup (Run Once):
   python -m venv venv
   source venv/bin/activate              # macOS/Linux
   # or venv\Scripts\activate            # Windows
   pip install -r requirements.txt
   cp .env.example .env
   mkdir -p data/documents

2. Configure:
   Edit .env (set USE_OPENAI, API keys, etc.)

3. Add Documents:
   Copy PDF/TXT files to data/documents/

4. Run (Terminal 1 - Backend):
   source venv/bin/activate
   python -m backend.api

5. Run (Terminal 2 - Frontend):
   source venv/bin/activate
   streamlit run frontend/app.py

6. Access:
   Frontend: http://localhost:8501
   API: http://localhost:8000
   Docs: http://localhost:8000/docs

7. Test:
   curl http://localhost:8000/health


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SUPPORT & RESOURCES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Documentation:
  â”œâ”€ README.md - Comprehensive overview
  â”œâ”€ deploy.md - Deployment guides
  â”œâ”€ SETUP_GUIDE.md - Configuration details
  â””â”€ PROJECT_SUMMARY.md - Architecture summary

API Documentation (Interactive):
  â””â”€ http://localhost:8000/docs (Swagger UI)

Python Libraries:
  â”œâ”€ FastAPI: https://fastapi.tiangolo.com/
  â”œâ”€ Streamlit: https://streamlit.io/
  â”œâ”€ LangChain: https://langchain.com/
  â”œâ”€ HuggingFace: https://huggingface.co/
  â””â”€ FAISS: https://github.com/facebookresearch/faiss

Community:
  â”œâ”€ HuggingFace Hub: https://huggingface.co/
  â”œâ”€ GitHub: https://github.com/
  â””â”€ Stack Overflow: https://stackoverflow.com/

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

FinBot is now ready for use! ğŸš€

Start with Steps 1-6 above, then refer to this checklist for additional setup.
If you encounter issues, check the Troubleshooting Guide (Section 9).

Good luck! ğŸ’°
